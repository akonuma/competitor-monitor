import requests
import hashlib
import json
import os
import difflib
import re
from datetime import datetime, timezone

# â”€â”€â”€ ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€ â”€â”€â”€
TARGET_URLS = json.loads(os.environ["TARGET_URLS"])
HASH_FILEÂ  Â = "hashes.json"
import requests
import hashlib
import json
import os
import difflib
import re
from datetime import datetime, timezone

# â”€â”€â”€ ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€ â”€â”€â”€
TARGET_URLS = json.loads(os.environ["TARGET_URLS"])
HASH_FILEÂ  Â = "hashes.json"
CONTENT_DIR = "content_cache"
TEAMS_WEBHOOK = os.environ["TEAMS_WEBHOOK"]


def load_hashes() -> dict:
Â  Â  """ãƒªãƒã‚¸ãƒˆãƒªä¸Šã®ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
Â  Â  if os.path.exists(HASH_FILE):
Â  Â  Â  Â  with open(HASH_FILE) as f:
Â  Â  Â  Â  Â  Â  return json.load(f)
Â  Â  return {}


def save_hashes(hashes: dict):
Â  Â  """ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã‚€"""
Â  Â  with open(HASH_FILE, "w") as f:
Â  Â  Â  Â  json.dump(hashes, f, indent=2)


def get_page_content(url: str) -> str | None:
Â  Â  """ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—"""
Â  Â  try:
Â  Â  Â  Â  resp = requests.get(url, timeout=15, headers={
Â  Â  Â  Â  Â  Â  "User-Agent": "Mozilla/5.0 (compatible; SiteMonitorBot/1.0)"
Â  Â  Â  Â  })
Â  Â  Â  Â  resp.raise_for_status()
Â  Â  Â  Â  return resp.text
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"[ERROR] {url} ã®å–å¾—ã«å¤±æ•—: {e}")
Â  Â  Â  Â  return None


def strip_html_tags(html: str) -> str:
Â  Â  """HTML ã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºï¼ˆç”»åƒURLãƒ»altå±æ€§ã‚‚å«ã‚€ï¼‰"""
Â  Â  # ç”»åƒã‚¿ã‚°ã‹ã‚‰ URL ã¨ alt ã‚’æŠ½å‡ºã—ã¦ä¿å­˜
Â  Â  images = []
Â  Â  for match in re.finditer(r'<img[^>]*>', html, re.IGNORECASE):
Â  Â  Â  Â  img_tag = match.group(0)
Â  Â  Â  Â  # src å±æ€§ã‚’æŠ½å‡º
Â  Â  Â  Â  src_match = re.search(r'src=["\']([^"\']+)["\']', img_tag, re.IGNORECASE)
Â  Â  Â  Â  # alt å±æ€§ã‚’æŠ½å‡º
Â  Â  Â  Â  alt_match = re.search(r'alt=["\']([^"\']*)["\']', img_tag, re.IGNORECASE)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if src_match:
Â  Â  Â  Â  Â  Â  src = src_match.group(1)
Â  Â  Â  Â  Â  Â  alt = alt_match.group(1) if alt_match else ""
Â  Â  Â  Â  Â  Â  images.append(f"[IMAGE: {src}]" + (f" (alt: {alt})" if alt else ""))
Â  Â Â 
Â  Â  # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
Â  Â  html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
Â  Â  html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
Â  Â  # ã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
Â  Â  html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)
Â  Â  # HTML ã‚¿ã‚°ã‚’é™¤å»
Â  Â  text = re.sub(r'<[^>]+>', '\n', html)Â  # ã‚¿ã‚°ã‚’æ”¹è¡Œã«ç½®æ›
Â  Â  # HTML ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
Â  Â  text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
Â  Â  text = text.replace('&quot;', '"').replace('&apos;', "'")
Â  Â Â 
Â  Â  # ç©ºç™½ãƒ»æ”¹è¡Œã‚’å®Œå…¨ã«æ­£è¦åŒ–
Â  Â  # 1. é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
Â  Â  text = re.sub(r'[ \t]+', ' ', text)
Â  Â  # 2. è¡Œã”ã¨ã«åˆ†å‰²ã—ã¦ãƒˆãƒªãƒ 
Â  Â  lines = [line.strip() for line in text.split('\n')]
Â  Â  # 3. ç©ºè¡Œã‚’é™¤å»
Â  Â  lines = [line for line in lines if line]
Â  Â  # 4. æ„å‘³ã®ã‚ã‚‹å˜ä½ï¼ˆæ–‡ã‚„æ®µè½ï¼‰ã§æ”¹è¡Œ
Â  Â  # å¥èª­ç‚¹ã®å¾Œã«é©åˆ‡ãªæ”¹è¡Œã‚’å…¥ã‚Œã‚‹
Â  Â  normalized_lines = []
Â  Â  for line in lines:
Â  Â  Â  Â  # é•·ã™ãã‚‹è¡Œã¯å¥èª­ç‚¹ã§åˆ†å‰²
Â  Â  Â  Â  if len(line) > 100:
Â  Â  Â  Â  Â  Â  # ã€‚ã‚„ï¼ï¼Ÿã§åˆ†å‰²
Â  Â  Â  Â  Â  Â  sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\.!?])', line)
Â  Â  Â  Â  Â  Â  current = ""
Â  Â  Â  Â  Â  Â  for i in range(0, len(sentences), 2):
Â  Â  Â  Â  Â  Â  Â  Â  sentence = sentences[i]
Â  Â  Â  Â  Â  Â  Â  Â  punct = sentences[i+1] if i+1 < len(sentences) else ""
Â  Â  Â  Â  Â  Â  Â  Â  current += sentence + punct
Â  Â  Â  Â  Â  Â  Â  Â  if len(current) > 80 or punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if current.strip():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  normalized_lines.append(current.strip())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current = ""
Â  Â  Â  Â  Â  Â  if current.strip():
Â  Â  Â  Â  Â  Â  Â  Â  normalized_lines.append(current.strip())
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  normalized_lines.append(line)
Â  Â Â 
Â  Â  # ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒæƒ…å ±ã‚’çµåˆ
Â  Â  if images:
Â  Â  Â  Â  normalized_lines.append("")
Â  Â  Â  Â  normalized_lines.append("--- ç”»åƒä¸€è¦§ ---")
Â  Â  Â  Â  normalized_lines.extend(images)
Â  Â Â 
Â  Â  return '\n'.join(normalized_lines)


def get_text_content_hash(html: str) -> str:
Â  Â  """ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆHTMLã‚¿ã‚°ã‚’é™¤å»å¾Œï¼‰"""
Â  Â  text = strip_html_tags(html)
Â  Â  return hashlib.md5(text.encode('utf-8')).hexdigest()


def save_content(url: str, content: str):
Â  Â  """ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
Â  Â  os.makedirs(CONTENT_DIR, exist_ok=True)
Â  Â  filename = hashlib.md5(url.encode('utf-8')).hexdigest() + ".txt"
Â  Â  filepath = os.path.join(CONTENT_DIR, filename)
Â  Â  with open(filepath, "w", encoding="utf-8") as f:
Â  Â  Â  Â  f.write(content)


def load_content(url: str) -> str | None:
Â  Â  """ä¿å­˜ã•ã‚ŒãŸãƒšãƒ¼ã‚¸å†…å®¹ã‚’èª­ã¿è¾¼ã‚€"""
Â  Â  filename = hashlib.md5(url.encode('utf-8')).hexdigest() + ".txt"
Â  Â  filepath = os.path.join(CONTENT_DIR, filename)
Â  Â  if os.path.exists(filepath):
Â  Â  Â  Â  with open(filepath, "r", encoding="utf-8") as f:
Â  Â  Â  Â  Â  Â  return f.read()
Â  Â  return None


def get_diff_summary(old_content: str, new_content: str, max_changes: int = 15) -> str:
Â  Â  """å¤‰æ›´ã®å·®åˆ†ã‚µãƒãƒªãƒ¼ã‚’å–å¾—ï¼ˆå¤‰æ›´ç®‡æ‰€ã®ã¿è¡¨ç¤ºï¼‰"""
Â  Â  old_lines = old_content.splitlines()
Â  Â  new_lines = new_content.splitlines()
Â  Â Â 
Â  Â  # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
Â  Â  if old_lines == new_lines:
Â  Â  Â  Â  return "å¤‰æ›´ãªã—"
Â  Â Â 
Â  Â  diff = list(difflib.unified_diff(
Â  Â  Â  Â  old_lines,Â 
Â  Â  Â  Â  new_lines,Â 
Â  Â  Â  Â  lineterm='',
Â  Â  Â  Â  n=0Â  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œãªã—ï¼ˆå¤‰æ›´ç®‡æ‰€ã®ã¿ï¼‰
Â  Â  ))
Â  Â Â 
Â  Â  if not diff or len(diff) <= 2:
Â  Â  Â  Â  return "å¤‰æ›´ãªã—"
Â  Â Â 
Â  Â  # å®Ÿéš›ã®å¤‰æ›´è¡Œã®ã¿æŠ½å‡º
Â  Â  added = []
Â  Â  removed = []
Â  Â Â 
Â  Â  for line in diff[2:]:Â  # æœ€åˆã®2è¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
Â  Â  Â  Â  if line.startswith('---') or line.startswith('+++') or line.startswith('@@'):
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  elif line.startswith('+'):
Â  Â  Â  Â  Â  Â  added.append(line[1:].strip())
Â  Â  Â  Â  elif line.startswith('-'):
Â  Â  Â  Â  Â  Â  removed.append(line[1:].strip())
Â  Â Â 
Â  Â  # å¤‰æ›´ãŒãªã„å ´åˆ
Â  Â  if not added and not removed:
Â  Â  Â  Â  return "å¤‰æ›´ãªã—"
Â  Â Â 
Â  Â  # åˆ†ã‹ã‚Šã‚„ã™ã„å½¢å¼ã§å‡ºåŠ›
Â  Â  result = []
Â  Â Â 
Â  Â  if removed:
Â  Â  Â  Â  result.append("ã€å‰Šé™¤ã•ã‚ŒãŸå†…å®¹ã€‘")
Â  Â  Â  Â  for i, line in enumerate(removed[:max_changes], 1):
Â  Â  Â  Â  Â  Â  if line:Â  # ç©ºè¡Œã¯é™¤å¤–
Â  Â  Â  Â  Â  Â  Â  Â  result.append(f"Â  - {line}")
Â  Â  Â  Â  if len(removed) > max_changes:
Â  Â  Â  Â  Â  Â  result.append(f"Â  ... ä»– {len(removed) - max_changes} è¡Œ")
Â  Â Â 
Â  Â  if added:
Â  Â  Â  Â  if removed:
Â  Â  Â  Â  Â  Â  result.append("")Â  # ç©ºè¡Œã§åŒºåˆ‡ã‚‹
Â  Â  Â  Â  result.append("ã€è¿½åŠ ã•ã‚ŒãŸå†…å®¹ã€‘")
Â  Â  Â  Â  for i, line in enumerate(added[:max_changes], 1):
Â  Â  Â  Â  Â  Â  if line:Â  # ç©ºè¡Œã¯é™¤å¤–
Â  Â  Â  Â  Â  Â  Â  Â  result.append(f"Â  + {line}")
Â  Â  Â  Â  if len(added) > max_changes:
Â  Â  Â  Â  Â  Â  result.append(f"Â  ... ä»– {len(added) - max_changes} è¡Œ")
Â  Â Â 
Â  Â  return '\n'.join(result) if result else "å¤‰æ›´ãªã—"


def send_teams_alert(changed_urls: list[dict]):
Â  Â  """å¤‰æ›´ã•ã‚ŒãŸURLã«ã¤ã„ã¦Teamsé€šçŸ¥ã‚’é€ä¿¡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†ã®ã¿ï¼‰"""
Â  Â  now = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")

Â  Â  sections = [{
Â  Â  Â  Â  "activityTitle": "å¤‰æ›´æ¤œçŸ¥ã‚µãƒãƒªãƒ¼",
Â  Â  Â  Â  "activitySubtitle": f"æ¤œçŸ¥æ™‚åˆ»: {now}",
Â  Â  Â  Â  "text": f"**{len(changed_urls)}ä»¶ã®ã‚µã‚¤ãƒˆã§å®Ÿè³ªçš„ãªå†…å®¹å¤‰æ›´ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸ**"
Â  Â  }]
Â  Â Â 
Â  Â  for item in changed_urls:
Â  Â  Â  Â  url = item["url"]
Â  Â  Â  Â  text_diff = item.get("text_diff", "å·®åˆ†æƒ…å ±ãªã—")
Â  Â  Â  Â Â 
Â  Â  Â  Â  sections.append({
Â  Â  Â  Â  Â  Â  "activityTitle": f"ğŸ“ {url}",
Â  Â  Â  Â  Â  Â  "activitySubtitle": "**å¤‰æ›´å†…å®¹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†ï¼‰**",
Â  Â  Â  Â  Â  Â  "text": f"```\n{text_diff[:1500]}\n```"
Â  Â  Â  Â  })

Â  Â  payload = {
Â  Â  Â  Â  "@type": "MessageCard",
Â  Â  Â  Â  "@context": "https://schema.org/extensions",
Â  Â  Â  Â  "summary": f"ç«¶åˆã‚µã‚¤ãƒˆæ›´æ–°æ¤œçŸ¥ ({len(changed_urls)}ä»¶)",
Â  Â  Â  Â  "themeColor": "0078D4",
Â  Â  Â  Â  "title": f"ğŸ”” ç«¶åˆã‚µã‚¤ãƒˆæ›´æ–°æ¤œçŸ¥ ({len(changed_urls)}ä»¶)",
Â  Â  Â  Â  "sections": sections
Â  Â  }

Â  Â  try:
Â  Â  Â  Â  resp = requests.post(TEAMS_WEBHOOK, json=payload, timeout=10)
Â  Â  Â  Â  resp.raise_for_status()
Â  Â  Â  Â  print(f"[OK] Teamsé€šçŸ¥é€ä¿¡å®Œäº†")
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"[ERROR] Teamsé€šçŸ¥é€ä¿¡å¤±æ•—: {e}")


def main():
Â  Â  print(f"[START] {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')} ãƒã‚§ãƒƒã‚¯é–‹å§‹")

Â  Â  hashes = load_hashes()
Â  Â  changed = []

Â  Â  for url in TARGET_URLS:
Â  Â  Â  Â  current_html = get_page_content(url)
Â  Â  Â  Â  if current_html is None:
Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆHTMLã‚¿ã‚°é™¤å»å¾Œï¼‰
Â  Â  Â  Â  current_hash = get_text_content_hash(current_html)
Â  Â  Â  Â  prev_hash = hashes.get(url)

Â  Â  Â  Â  if prev_hash is None:
Â  Â  Â  Â  Â  Â  # åˆå›ç™»éŒ²
Â  Â  Â  Â  Â  Â  print(f"[NEW]Â  Â  Â {url}")
Â  Â  Â  Â  Â  Â  hashes[url] = current_hash
Â  Â  Â  Â  Â  Â  save_content(url, current_html)
Â  Â  Â  Â  elif current_hash != prev_hash:
Â  Â  Â  Â  Â  Â  # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå¤‰æ›´ã•ã‚ŒãŸ
Â  Â  Â  Â  Â  Â  print(f"[CHANGED] {url}")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  old_html = load_content(url)
Â  Â  Â  Â  Â  Â  text_diff = "å‰å›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if old_html:
Â  Â  Â  Â  Â  Â  Â  Â  # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºã—ã¦å·®åˆ†ã‚’ä½œæˆ
Â  Â  Â  Â  Â  Â  Â  Â  old_text = strip_html_tags(old_html)
Â  Â  Â  Â  Â  Â  Â  Â  new_text = strip_html_tags(current_html)
Â  Â  Â  Â  Â  Â  Â  Â  text_diff = get_diff_summary(old_text, new_text, max_changes=20)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  changed.append({
Â  Â  Â  Â  Â  Â  Â  Â  "url": url,
Â  Â  Â  Â  Â  Â  Â  Â  "text_diff": text_diff
Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  hashes[url] = current_hash
Â  Â  Â  Â  Â  Â  save_content(url, current_html)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  print(f"[OK]Â  Â  Â  {url} (ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¤‰æ›´ãªã—)")

Â  Â  save_hashes(hashes)

Â  Â  if changed:
Â  Â  Â  Â  send_teams_alert(changed)
Â  Â  else:
Â  Â  Â  Â  print("[INFO] å®Ÿè³ªçš„ãªå¤‰æ›´ãªã—")

Â  Â  print("[END] ãƒã‚§ãƒƒã‚¯å®Œäº†")


if __name__ == "__main__":
Â  Â  main()
CONTENT_DIR = "content_cache"
TEAMS_WEBHOOK = os.environ["TEAMS_WEBHOOK"]


def load_hashes() -> dict:
Â  Â  """ãƒªãƒã‚¸ãƒˆãƒªä¸Šã®ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
Â  Â  if os.path.exists(HASH_FILE):
Â  Â  Â  Â  with open(HASH_FILE) as f:
Â  Â  Â  Â  Â  Â  return json.load(f)
Â  Â  return {}


def save_hashes(hashes: dict):
Â  Â  """ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã‚€"""
Â  Â  with open(HASH_FILE, "w") as f:
Â  Â  Â  Â  json.dump(hashes, f, indent=2)


def get_page_content(url: str) -> str | None:
Â  Â  """ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—"""
Â  Â  try:
Â  Â  Â  Â  resp = requests.get(url, timeout=15, headers={
Â  Â  Â  Â  Â  Â  "User-Agent": "Mozilla/5.0 (compatible; SiteMonitorBot/1.0)"
Â  Â  Â  Â  })
Â  Â  Â  Â  resp.raise_for_status()
Â  Â  Â  Â  return resp.text
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"[ERROR] {url} ã®å–å¾—ã«å¤±æ•—: {e}")
Â  Â  Â  Â  return None


def strip_html_tags(html: str) -> str:
Â  Â  """HTML ã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºï¼ˆç”»åƒURLãƒ»altå±æ€§ã‚‚å«ã‚€ï¼‰"""
Â  Â  # ç”»åƒã‚¿ã‚°ã‹ã‚‰ URL ã¨ alt ã‚’æŠ½å‡ºã—ã¦ä¿å­˜
Â  Â  images = []
Â  Â  for match in re.finditer(r'<img[^>]*>', html, re.IGNORECASE):
Â  Â  Â  Â  img_tag = match.group(0)
Â  Â  Â  Â  # src å±æ€§ã‚’æŠ½å‡º
Â  Â  Â  Â  src_match = re.search(r'src=["\']([^"\']+)["\']', img_tag, re.IGNORECASE)
Â  Â  Â  Â  # alt å±æ€§ã‚’æŠ½å‡º
Â  Â  Â  Â  alt_match = re.search(r'alt=["\']([^"\']*)["\']', img_tag, re.IGNORECASE)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if src_match:
Â  Â  Â  Â  Â  Â  src = src_match.group(1)
Â  Â  Â  Â  Â  Â  alt = alt_match.group(1) if alt_match else ""
Â  Â  Â  Â  Â  Â  images.append(f"[IMAGE: {src}]" + (f" (alt: {alt})" if alt else ""))
Â  Â Â 
Â  Â  # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
Â  Â  html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
Â  Â  html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
Â  Â  # ã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
Â  Â  html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)
Â  Â  # HTML ã‚¿ã‚°ã‚’é™¤å»
Â  Â  text = re.sub(r'<[^>]+>', '', html)
Â  Â  # HTML ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
Â  Â  text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
Â  Â  # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
Â  Â  text = re.sub(r'\s+', ' ', text)
Â  Â  # å„è¡Œã‚’ãƒˆãƒªãƒ 
Â  Â  lines = [line.strip() for line in text.split('\n') if line.strip()]
Â  Â Â 
Â  Â  # ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒæƒ…å ±ã‚’çµåˆ
Â  Â  if images:
Â  Â  Â  Â  lines.append("\n--- ç”»åƒä¸€è¦§ ---")
Â  Â  Â  Â  lines.extend(images)
Â  Â Â 
Â  Â  return '\n'.join(lines)


def get_text_content_hash(html: str) -> str:
Â  Â  """ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆHTMLã‚¿ã‚°ã‚’é™¤å»å¾Œï¼‰"""
Â  Â  text = strip_html_tags(html)
Â  Â  return hashlib.md5(text.encode('utf-8')).hexdigest()


def save_content(url: str, content: str):
Â  Â  """ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
Â  Â  os.makedirs(CONTENT_DIR, exist_ok=True)
Â  Â  filename = hashlib.md5(url.encode('utf-8')).hexdigest() + ".txt"
Â  Â  filepath = os.path.join(CONTENT_DIR, filename)
Â  Â  with open(filepath, "w", encoding="utf-8") as f:
Â  Â  Â  Â  f.write(content)


def load_content(url: str) -> str | None:
Â  Â  """ä¿å­˜ã•ã‚ŒãŸãƒšãƒ¼ã‚¸å†…å®¹ã‚’èª­ã¿è¾¼ã‚€"""
Â  Â  filename = hashlib.md5(url.encode('utf-8')).hexdigest() + ".txt"
Â  Â  filepath = os.path.join(CONTENT_DIR, filename)
Â  Â  if os.path.exists(filepath):
Â  Â  Â  Â  with open(filepath, "r", encoding="utf-8") as f:
Â  Â  Â  Â  Â  Â  return f.read()
Â  Â  return None


def get_diff_summary(old_content: str, new_content: str, max_lines: int = 20) -> str:
Â  Â  """å¤‰æ›´ã®å·®åˆ†ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
Â  Â  old_lines = old_content.splitlines()
Â  Â  new_lines = new_content.splitlines()
Â  Â Â 
Â  Â  diff = list(difflib.unified_diff(
Â  Â  Â  Â  old_lines,Â 
Â  Â  Â  Â  new_lines,Â 
Â  Â  Â  Â  lineterm='',
Â  Â  Â  Â  n=1Â  # å‰å¾Œ1è¡Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
Â  Â  ))
Â  Â Â 
Â  Â  if not diff:
Â  Â  Â  Â  return "å¤‰æ›´ãªã—"
Â  Â Â 
Â  Â  changes = []
Â  Â  for line in diff[2:]:Â  # æœ€åˆã®2è¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
Â  Â  Â  Â  if line.startswith('---') or line.startswith('+++'):
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  changes.append(line)
Â  Â Â 
Â  Â  if len(changes) > max_lines:
Â  Â  Â  Â  changes = changes[:max_lines]
Â  Â  Â  Â  changes.append(f"... (ä»– {len(changes) - max_lines} è¡Œä»¥ä¸Š)")
Â  Â Â 
Â  Â  return '\n'.join(changes) if changes else "å·®åˆ†ãªã—"


def send_teams_alert(changed_urls: list[dict]):
Â  Â  """å¤‰æ›´ã•ã‚ŒãŸURLã«ã¤ã„ã¦Teamsé€šçŸ¥ã‚’é€ä¿¡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†ã®ã¿ï¼‰"""
Â  Â  now = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")

Â  Â  sections = [{
Â  Â  Â  Â  "activityTitle": "å¤‰æ›´æ¤œçŸ¥ã‚µãƒãƒªãƒ¼",
Â  Â  Â  Â  "activitySubtitle": f"æ¤œçŸ¥æ™‚åˆ»: {now}",
Â  Â  Â  Â  "text": f"**{len(changed_urls)}ä»¶ã®ã‚µã‚¤ãƒˆã§å®Ÿè³ªçš„ãªå†…å®¹å¤‰æ›´ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸ**"
Â  Â  }]
Â  Â Â 
Â  Â  for item in changed_urls:
Â  Â  Â  Â  url = item["url"]
Â  Â  Â  Â  text_diff = item.get("text_diff", "å·®åˆ†æƒ…å ±ãªã—")
Â  Â  Â  Â Â 
Â  Â  Â  Â  sections.append({
Â  Â  Â  Â  Â  Â  "activityTitle": f"ğŸ“ {url}",
Â  Â  Â  Â  Â  Â  "activitySubtitle": "**å¤‰æ›´å†…å®¹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†ï¼‰**",
Â  Â  Â  Â  Â  Â  "text": f"```\n{text_diff[:1500]}\n```"
Â  Â  Â  Â  })

Â  Â  payload = {
Â  Â  Â  Â  "@type": "MessageCard",
Â  Â  Â  Â  "@context": "https://schema.org/extensions",
Â  Â  Â  Â  "summary": f"ç«¶åˆã‚µã‚¤ãƒˆæ›´æ–°æ¤œçŸ¥ ({len(changed_urls)}ä»¶)",
Â  Â  Â  Â  "themeColor": "0078D4",
Â  Â  Â  Â  "title": f"ğŸ”” ç«¶åˆã‚µã‚¤ãƒˆæ›´æ–°æ¤œçŸ¥ ({len(changed_urls)}ä»¶)",
Â  Â  Â  Â  "sections": sections
Â  Â  }

Â  Â  try:
Â  Â  Â  Â  resp = requests.post(TEAMS_WEBHOOK, json=payload, timeout=10)
Â  Â  Â  Â  resp.raise_for_status()
Â  Â  Â  Â  print(f"[OK] Teamsé€šçŸ¥é€ä¿¡å®Œäº†")
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"[ERROR] Teamsé€šçŸ¥é€ä¿¡å¤±æ•—: {e}")


def main():
Â  Â  print(f"[START] {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')} ãƒã‚§ãƒƒã‚¯é–‹å§‹")

Â  Â  hashes = load_hashes()
Â  Â  changed = []

Â  Â  for url in TARGET_URLS:
Â  Â  Â  Â  current_html = get_page_content(url)
Â  Â  Â  Â  if current_html is None:
Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆHTMLã‚¿ã‚°é™¤å»å¾Œï¼‰
Â  Â  Â  Â  current_hash = get_text_content_hash(current_html)
Â  Â  Â  Â  prev_hash = hashes.get(url)

Â  Â  Â  Â  if prev_hash is None:
Â  Â  Â  Â  Â  Â  # åˆå›ç™»éŒ²
Â  Â  Â  Â  Â  Â  print(f"[NEW]Â  Â  Â {url}")
Â  Â  Â  Â  Â  Â  hashes[url] = current_hash
Â  Â  Â  Â  Â  Â  save_content(url, current_html)
Â  Â  Â  Â  elif current_hash != prev_hash:
Â  Â  Â  Â  Â  Â  # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå¤‰æ›´ã•ã‚ŒãŸ
Â  Â  Â  Â  Â  Â  print(f"[CHANGED] {url}")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  old_html = load_content(url)
Â  Â  Â  Â  Â  Â  text_diff = "å‰å›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if old_html:
Â  Â  Â  Â  Â  Â  Â  Â  # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºã—ã¦å·®åˆ†ã‚’ä½œæˆ
Â  Â  Â  Â  Â  Â  Â  Â  old_text = strip_html_tags(old_html)
Â  Â  Â  Â  Â  Â  Â  Â  new_text = strip_html_tags(current_html)
Â  Â  Â  Â  Â  Â  Â  Â  text_diff = get_diff_summary(old_text, new_text, max_lines=30)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  changed.append({
Â  Â  Â  Â  Â  Â  Â  Â  "url": url,
Â  Â  Â  Â  Â  Â  Â  Â  "text_diff": text_diff
Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  hashes[url] = current_hash
Â  Â  Â  Â  Â  Â  save_content(url, current_html)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  print(f"[OK]Â  Â  Â  {url} (ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¤‰æ›´ãªã—)")

Â  Â  save_hashes(hashes)

Â  Â  if changed:
Â  Â  Â  Â  send_teams_alert(changed)
Â  Â  else:
Â  Â  Â  Â  print("[INFO] å®Ÿè³ªçš„ãªå¤‰æ›´ãªã—")

Â  Â  print("[END] ãƒã‚§ãƒƒã‚¯å®Œäº†")


if __name__ == "__main__":
Â  Â  main()
