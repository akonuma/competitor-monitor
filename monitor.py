import requests
import hashlib
import json
import os
import difflib
import re
from datetime import datetime, timezone

# â”€â”€â”€ ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€ â”€â”€â”€
# TARGET_URLS ã¯ ['https://...'] ã®å½¢å¼ã®JSONæ–‡å­—åˆ—ã‚’æƒ³å®š
TARGET_URLS = json.loads(os.environ.get("TARGET_URLS", "[]"))
HASH_FILE    = "hashes.json"
CONTENT_DIR = "content_cache"
TEAMS_WEBHOOK = os.environ.get("TEAMS_WEBHOOK", "")


def load_hashes() -> dict:
    """ãƒªãƒã‚¸ãƒˆãƒªä¸Šã®ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(HASH_FILE):
        with open(HASH_FILE) as f:
            try:
                return json.load(f)
            except:
                return {}
    return {}


def save_hashes(hashes: dict):
    """ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã‚€"""
    with open(HASH_FILE, "w") as f:
        json.dump(hashes, f, indent=2)


def get_page_content(url: str) -> str | None:
    """ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—"""
    try:
        resp = requests.get(url, timeout=15, headers={
            "User-Agent": "Mozilla/5.0 (compatible; SiteMonitorBot/1.0)"
        })
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        print(f"[ERROR] {url} ã®å–å¾—ã«å¤±æ•—: {e}")
        return None


def strip_html_tags(html: str) -> str:
    """HTML ã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºï¼ˆç”»åƒURLãƒ»altå±æ€§ã‚‚å«ã‚€ï¼‰"""
    images = []
    for match in re.finditer(r'<img[^>]*>', html, re.IGNORECASE):
        img_tag = match.group(0)
        src_match = re.search(r'src=["\']([^"\']+)["\']', img_tag, re.IGNORECASE)
        alt_match = re.search(r'alt=["\']([^"\']*)["\']', img_tag, re.IGNORECASE)
        
        if src_match:
            src = src_match.group(1)
            alt = alt_match.group(1) if alt_match else ""
            # ç‰¹å®šã®å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç­‰ï¼‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ç„¡è¦–ã™ã‚‹ç­‰ã®å‡¦ç†ãŒå¿…è¦ãªå ´åˆã‚‚ã‚ã‚‹ãŒã€ä¸€æ—¦ãã®ã¾ã¾
            images.append(f"[IMAGE: {src}]" + (f" (alt: {alt})" if alt else ""))
    
    html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)
    
    text = re.sub(r'<[^>]+>', '\n', html)
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    text = text.replace('&quot;', '"').replace('&apos;', "'")
    
    text = re.sub(r'[ \t]+', ' ', text)
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    
    if images:
        lines.append("")
        lines.append("--- ç”»åƒä¸€è¦§ ---")
        lines.extend(images)
    
    return '\n'.join(lines)


def get_text_content_hash(html: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
    text = strip_html_tags(html)
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def save_content(url: str, content: str):
    """ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    os.makedirs(CONTENT_DIR, exist_ok=True)
    filename = hashlib.md5(url.encode('utf-8')).hexdigest() + ".txt"
    filepath = os.path.join(CONTENT_DIR, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)


def load_content(url: str) -> str | None:
    """ä¿å­˜ã•ã‚ŒãŸãƒšãƒ¼ã‚¸å†…å®¹ã‚’èª­ã¿è¾¼ã‚€"""
    filename = hashlib.md5(url.encode('utf-8')).hexdigest() + ".txt"
    filepath = os.path.join(CONTENT_DIR, filename)
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    return None


def get_diff_summary(old_content: str, new_content: str, max_changes: int = 15) -> str:
    """å¤‰æ›´ã®å·®åˆ†ã‚µãƒãƒªãƒ¼ã‚’å–å¾—ã€‚æœ¬å½“ã«å¤‰æ›´ãŒã‚ã‚‹å ´åˆã®ã¿æ–‡å­—åˆ—ã‚’è¿”ã™"""
    old_lines = old_content.splitlines()
    new_lines = new_content.splitlines()
    
    if old_lines == new_lines:
        return "å¤‰æ›´ãªã—"
    
    diff = list(difflib.unified_diff(old_lines, new_lines, lineterm='', n=0))
    
    added = []
    removed = []
    for line in diff[2:]:
        if line.startswith('+') and not line.startswith('+++'):
            added.append(line[1:].strip())
        elif line.startswith('-') and not line.startswith('---'):
            removed.append(line[1:].strip())
    
    if not added and not removed:
        return "å¤‰æ›´ãªã—"
    
    result = []
    if removed:
        result.append("ã€å‰Šé™¤ã•ã‚ŒãŸå†…å®¹ã€‘")
        for line in removed[:max_changes]:
            if line: result.append(f"  - {line}")
        if len(removed) > max_changes: result.append(f"  ... ä»– {len(removed) - max_changes} è¡Œ")
    
    if added:
        if removed: result.append("")
        result.append("ã€è¿½åŠ ã•ã‚ŒãŸå†…å®¹ã€‘")
        for line in added[:max_changes]:
            if line: result.append(f"  + {line}")
        if len(added) > max_changes: result.append(f"  ... ä»– {len(added) - max_changes} è¡Œ")
    
    return '\n'.join(result)


def send_teams_alert(changed_urls: list[dict]):
    """Teamsé€šçŸ¥ã‚’é€ä¿¡"""
    if not TEAMS_WEBHOOK:
        print("[ERROR] TEAMS_WEBHOOK ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    now = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
    sections = [{
        "activityTitle": "å¤‰æ›´æ¤œçŸ¥ã‚µãƒãƒªãƒ¼",
        "activitySubtitle": f"æ¤œçŸ¥æ™‚åˆ»: {now}",
        "text": f"**{len(changed_urls)}ä»¶ã®ã‚µã‚¤ãƒˆã§å®Ÿè³ªçš„ãªå†…å®¹å¤‰æ›´ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸ**"
    }]
    
    for item in changed_urls:
        sections.append({
            "activityTitle": f"ğŸ“ {item['url']}",
            "activitySubtitle": "**å¤‰æ›´å†…å®¹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†ï¼‰**",
            "text": f"
