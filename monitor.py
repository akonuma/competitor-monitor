import requests
import hashlib
import json
import os
import difflib
import re
from datetime import datetime, timezone

# â”€â”€â”€ ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€ â”€â”€â”€
TARGET_URLS = json.loads(os.environ["TARGET_URLS"])
HASH_FILE   = "hashes.json"import requests
import hashlib
import json
import os
import difflib
import re
from datetime import datetime, timezone

# â”€â”€â”€ ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€ â”€â”€â”€
TARGET_URLS = json.loads(os.environ["TARGET_URLS"])
HASH_FILE   = "hashes.json"
CONTENT_DIR = "content_cache"
TEAMS_WEBHOOK = os.environ["TEAMS_WEBHOOK"]


def load_hashes() -> dict:
    """ãƒªãƒã‚¸ãƒˆãƒªä¸Šã®ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(HASH_FILE):
        with open(HASH_FILE) as f:
            return json.load(f)
    return {}


def save_hashes(hashes: dict):
    """ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã‚€"""
    with open(HASH_FILE, "w") as f:
        json.dump(hashes, f, indent=2)


def get_page_content(url: str) -> str | None:
    """ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—"""
    try:
        resp = requests.get(url, timeout=15, headers={
            "User-Agent": "Mozilla/5.0 (compatible; SiteMonitorBot/1.0)"
        })
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        print(f"[ERROR] {url} ã®å–å¾—ã«å¤±æ•—: {e}")
        return None


def strip_html_tags(html: str) -> str:
    """HTML ã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºï¼ˆç”»åƒURLãƒ»altå±æ€§ã‚‚å«ã‚€ï¼‰"""
    # ç”»åƒã‚¿ã‚°ã‹ã‚‰ URL ã¨ alt ã‚’æŠ½å‡ºã—ã¦ä¿å­˜
    images = []
    for match in re.finditer(r'<img[^>]*>', html, re.IGNORECASE):
        img_tag = match.group(0)
        # src å±æ€§ã‚’æŠ½å‡º
        src_match = re.search(r'src=["\']([^"\']+)["\']', img_tag, re.IGNORECASE)
        # alt å±æ€§ã‚’æŠ½å‡º
        alt_match = re.search(r'alt=["\']([^"\']*)["\']', img_tag, re.IGNORECASE)
        
        if src_match:
            src = src_match.group(1)
            alt = alt_match.group(1) if alt_match else ""
            images.append(f"[IMAGE: {src}]" + (f" (alt: {alt})" if alt else ""))
    
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
    html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
    # ã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
    html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)
    # HTML ã‚¿ã‚°ã‚’é™¤å»
    text = re.sub(r'<[^>]+>', '\n', html)  # ã‚¿ã‚°ã‚’æ”¹è¡Œã«ç½®æ›
    # HTML ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    text = text.replace('&quot;', '"').replace('&apos;', "'")
    
    # ç©ºç™½ãƒ»æ”¹è¡Œã‚’å®Œå…¨ã«æ­£è¦åŒ–
    # 1. é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
    text = re.sub(r'[ \t]+', ' ', text)
    # 2. è¡Œã”ã¨ã«åˆ†å‰²ã—ã¦ãƒˆãƒªãƒ 
    lines = [line.strip() for line in text.split('\n')]
    # 3. ç©ºè¡Œã‚’é™¤å»
    lines = [line for line in lines if line]
    # 4. æ„å‘³ã®ã‚ã‚‹å˜ä½ï¼ˆæ–‡ã‚„æ®µè½ï¼‰ã§æ”¹è¡Œ
    # å¥èª­ç‚¹ã®å¾Œã«é©åˆ‡ãªæ”¹è¡Œã‚’å…¥ã‚Œã‚‹
    normalized_lines = []
    for line in lines:
        # é•·ã™ãã‚‹è¡Œã¯å¥èª­ç‚¹ã§åˆ†å‰²
        if len(line) > 100:
            # ã€‚ã‚„ï¼ï¼Ÿã§åˆ†å‰²
            sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\.!?])', line)
            current = ""
            for i in range(0, len(sentences), 2):
                sentence = sentences[i]
                punct = sentences[i+1] if i+1 < len(sentences) else ""
                current += sentence + punct
                if len(current) > 80 or punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']:
                    if current.strip():
                        normalized_lines.append(current.strip())
                    current = ""
            if current.strip():
                normalized_lines.append(current.strip())
        else:
            normalized_lines.append(line)
    
    # ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒæƒ…å ±ã‚’çµåˆ
    if images:
        normalized_lines.append("")
        normalized_lines.append("--- ç”»åƒä¸€è¦§ ---")
        normalized_lines.extend(images)
    
    return '\n'.join(normalized_lines)


def get_text_content_hash(html: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆHTMLã‚¿ã‚°ã‚’é™¤å»å¾Œï¼‰"""
    text = strip_html_tags(html)
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def save_content(url: str, content: str):
    """ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    os.makedirs(CONTENT_DIR, exist_ok=True)
    filename = hashlib.md5(url.encode('utf-8')).hexdigest() + ".txt"
    filepath = os.path.join(CONTENT_DIR, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)


def load_content(url: str) -> str | None:
    """ä¿å­˜ã•ã‚ŒãŸãƒšãƒ¼ã‚¸å†…å®¹ã‚’èª­ã¿è¾¼ã‚€"""
    filename = hashlib.md5(url.encode('utf-8')).hexdigest() + ".txt"
    filepath = os.path.join(CONTENT_DIR, filename)
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    return None


def get_diff_summary(old_content: str, new_content: str, max_changes: int = 15) -> str:
    """å¤‰æ›´ã®å·®åˆ†ã‚µãƒãƒªãƒ¼ã‚’å–å¾—ï¼ˆå¤‰æ›´ç®‡æ‰€ã®ã¿è¡¨ç¤ºï¼‰"""
    old_lines = old_content.splitlines()
    new_lines = new_content.splitlines()
    
    # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
    if old_lines == new_lines:
        return "å¤‰æ›´ãªã—"
    
    diff = list(difflib.unified_diff(
        old_lines, 
        new_lines, 
        lineterm='',
        n=0  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œãªã—ï¼ˆå¤‰æ›´ç®‡æ‰€ã®ã¿ï¼‰
    ))
    
    if not diff or len(diff) <= 2:
        return "å¤‰æ›´ãªã—"
    
    # å®Ÿéš›ã®å¤‰æ›´è¡Œã®ã¿æŠ½å‡º
    added = []
    removed = []
    
    for line in diff[2:]:  # æœ€åˆã®2è¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
        if line.startswith('---') or line.startswith('+++') or line.startswith('@@'):
            continue
        elif line.startswith('+'):
            added.append(line[1:].strip())
        elif line.startswith('-'):
            removed.append(line[1:].strip())
    
    # å¤‰æ›´ãŒãªã„å ´åˆ
    if not added and not removed:
        return "å¤‰æ›´ãªã—"
    
    # åˆ†ã‹ã‚Šã‚„ã™ã„å½¢å¼ã§å‡ºåŠ›
    result = []
    
    if removed:
        result.append("ã€å‰Šé™¤ã•ã‚ŒãŸå†…å®¹ã€‘")
        for i, line in enumerate(removed[:max_changes], 1):
            if line:  # ç©ºè¡Œã¯é™¤å¤–
                result.append(f"  - {line}")
        if len(removed) > max_changes:
            result.append(f"  ... ä»– {len(removed) - max_changes} è¡Œ")
    
    if added:
        if removed:
            result.append("")  # ç©ºè¡Œã§åŒºåˆ‡ã‚‹
        result.append("ã€è¿½åŠ ã•ã‚ŒãŸå†…å®¹ã€‘")
        for i, line in enumerate(added[:max_changes], 1):
            if line:  # ç©ºè¡Œã¯é™¤å¤–
                result.append(f"  + {line}")
        if len(added) > max_changes:
            result.append(f"  ... ä»– {len(added) - max_changes} è¡Œ")
    
    return '\n'.join(result) if result else "å¤‰æ›´ãªã—"


def send_teams_alert(changed_urls: list[dict]):
    """å¤‰æ›´ã•ã‚ŒãŸURLã«ã¤ã„ã¦Teamsé€šçŸ¥ã‚’é€ä¿¡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†ã®ã¿ï¼‰"""
    now = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")

    sections = [{
        "activityTitle": "å¤‰æ›´æ¤œçŸ¥ã‚µãƒãƒªãƒ¼",
        "activitySubtitle": f"æ¤œçŸ¥æ™‚åˆ»: {now}",
        "text": f"**{len(changed_urls)}ä»¶ã®ã‚µã‚¤ãƒˆã§å®Ÿè³ªçš„ãªå†…å®¹å¤‰æ›´ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸ**"
    }]
    
    for item in changed_urls:
        url = item["url"]
        text_diff = item.get("text_diff", "å·®åˆ†æƒ…å ±ãªã—")
        
        sections.append({
            "activityTitle": f"ğŸ“ {url}",
            "activitySubtitle": "**å¤‰æ›´å†…å®¹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†ï¼‰**",
            "text": f"```\n{text_diff[:1500]}\n```"
        })

    payload = {
        "@type": "MessageCard",
        "@context": "https://schema.org/extensions",
        "summary": f"ç«¶åˆã‚µã‚¤ãƒˆæ›´æ–°æ¤œçŸ¥ ({len(changed_urls)}ä»¶)",
        "themeColor": "0078D4",
        "title": f"ğŸ”” ç«¶åˆã‚µã‚¤ãƒˆæ›´æ–°æ¤œçŸ¥ ({len(changed_urls)}ä»¶)",
        "sections": sections
    }

    try:
        resp = requests.post(TEAMS_WEBHOOK, json=payload, timeout=10)
        resp.raise_for_status()
        print(f"[OK] Teamsé€šçŸ¥é€ä¿¡å®Œäº†")
    except Exception as e:
        print(f"[ERROR] Teamsé€šçŸ¥é€ä¿¡å¤±æ•—: {e}")


def main():
    print(f"[START] {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')} ãƒã‚§ãƒƒã‚¯é–‹å§‹")

    hashes = load_hashes()
    changed = []

    for url in TARGET_URLS:
        current_html = get_page_content(url)
        if current_html is None:
            continue

        # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆHTMLã‚¿ã‚°é™¤å»å¾Œï¼‰
        current_hash = get_text_content_hash(current_html)
        prev_hash = hashes.get(url)

        if prev_hash is None:
            # åˆå›ç™»éŒ²
            print(f"[NEW]     {url}")
            hashes[url] = current_hash
            save_content(url, current_html)
        elif current_hash != prev_hash:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå¤‰æ›´ã•ã‚ŒãŸ
            print(f"[CHANGED] {url}")
            
            old_html = load_content(url)
            text_diff = "å‰å›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            
            if old_html:
                # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºã—ã¦å·®åˆ†ã‚’ä½œæˆ
                old_text = strip_html_tags(old_html)
                new_text = strip_html_tags(current_html)
                text_diff = get_diff_summary(old_text, new_text, max_changes=20)
            
            changed.append({
                "url": url,
                "text_diff": text_diff
            })
            
            hashes[url] = current_hash
            save_content(url, current_html)
        else:
            print(f"[OK]      {url} (ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¤‰æ›´ãªã—)")

    save_hashes(hashes)

    if changed:
        send_teams_alert(changed)
    else:
        print("[INFO] å®Ÿè³ªçš„ãªå¤‰æ›´ãªã—")

    print("[END] ãƒã‚§ãƒƒã‚¯å®Œäº†")


if __name__ == "__main__":
    main()
CONTENT_DIR = "content_cache"
TEAMS_WEBHOOK = os.environ["TEAMS_WEBHOOK"]


def load_hashes() -> dict:
    """ãƒªãƒã‚¸ãƒˆãƒªä¸Šã®ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(HASH_FILE):
        with open(HASH_FILE) as f:
            return json.load(f)
    return {}


def save_hashes(hashes: dict):
    """ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã‚€"""
    with open(HASH_FILE, "w") as f:
        json.dump(hashes, f, indent=2)


def get_page_content(url: str) -> str | None:
    """ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—"""
    try:
        resp = requests.get(url, timeout=15, headers={
            "User-Agent": "Mozilla/5.0 (compatible; SiteMonitorBot/1.0)"
        })
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        print(f"[ERROR] {url} ã®å–å¾—ã«å¤±æ•—: {e}")
        return None


def strip_html_tags(html: str) -> str:
    """HTML ã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºï¼ˆç”»åƒURLãƒ»altå±æ€§ã‚‚å«ã‚€ï¼‰"""
    # ç”»åƒã‚¿ã‚°ã‹ã‚‰ URL ã¨ alt ã‚’æŠ½å‡ºã—ã¦ä¿å­˜
    images = []
    for match in re.finditer(r'<img[^>]*>', html, re.IGNORECASE):
        img_tag = match.group(0)
        # src å±æ€§ã‚’æŠ½å‡º
        src_match = re.search(r'src=["\']([^"\']+)["\']', img_tag, re.IGNORECASE)
        # alt å±æ€§ã‚’æŠ½å‡º
        alt_match = re.search(r'alt=["\']([^"\']*)["\']', img_tag, re.IGNORECASE)
        
        if src_match:
            src = src_match.group(1)
            alt = alt_match.group(1) if alt_match else ""
            images.append(f"[IMAGE: {src}]" + (f" (alt: {alt})" if alt else ""))
    
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
    html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
    # ã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
    html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)
    # HTML ã‚¿ã‚°ã‚’é™¤å»
    text = re.sub(r'<[^>]+>', '', html)
    # HTML ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
    text = re.sub(r'\s+', ' ', text)
    # å„è¡Œã‚’ãƒˆãƒªãƒ 
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    
    # ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒæƒ…å ±ã‚’çµåˆ
    if images:
        lines.append("\n--- ç”»åƒä¸€è¦§ ---")
        lines.extend(images)
    
    return '\n'.join(lines)


def get_text_content_hash(html: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆHTMLã‚¿ã‚°ã‚’é™¤å»å¾Œï¼‰"""
    text = strip_html_tags(html)
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def save_content(url: str, content: str):
    """ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    os.makedirs(CONTENT_DIR, exist_ok=True)
    filename = hashlib.md5(url.encode('utf-8')).hexdigest() + ".txt"
    filepath = os.path.join(CONTENT_DIR, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)


def load_content(url: str) -> str | None:
    """ä¿å­˜ã•ã‚ŒãŸãƒšãƒ¼ã‚¸å†…å®¹ã‚’èª­ã¿è¾¼ã‚€"""
    filename = hashlib.md5(url.encode('utf-8')).hexdigest() + ".txt"
    filepath = os.path.join(CONTENT_DIR, filename)
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    return None


def get_diff_summary(old_content: str, new_content: str, max_lines: int = 20) -> str:
    """å¤‰æ›´ã®å·®åˆ†ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
    old_lines = old_content.splitlines()
    new_lines = new_content.splitlines()
    
    diff = list(difflib.unified_diff(
        old_lines, 
        new_lines, 
        lineterm='',
        n=1  # å‰å¾Œ1è¡Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
    ))
    
    if not diff:
        return "å¤‰æ›´ãªã—"
    
    changes = []
    for line in diff[2:]:  # æœ€åˆã®2è¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
        if line.startswith('---') or line.startswith('+++'):
            continue
        changes.append(line)
    
    if len(changes) > max_lines:
        changes = changes[:max_lines]
        changes.append(f"... (ä»– {len(changes) - max_lines} è¡Œä»¥ä¸Š)")
    
    return '\n'.join(changes) if changes else "å·®åˆ†ãªã—"


def send_teams_alert(changed_urls: list[dict]):
    """å¤‰æ›´ã•ã‚ŒãŸURLã«ã¤ã„ã¦Teamsé€šçŸ¥ã‚’é€ä¿¡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†ã®ã¿ï¼‰"""
    now = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")

    sections = [{
        "activityTitle": "å¤‰æ›´æ¤œçŸ¥ã‚µãƒãƒªãƒ¼",
        "activitySubtitle": f"æ¤œçŸ¥æ™‚åˆ»: {now}",
        "text": f"**{len(changed_urls)}ä»¶ã®ã‚µã‚¤ãƒˆã§å®Ÿè³ªçš„ãªå†…å®¹å¤‰æ›´ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸ**"
    }]
    
    for item in changed_urls:
        url = item["url"]
        text_diff = item.get("text_diff", "å·®åˆ†æƒ…å ±ãªã—")
        
        sections.append({
            "activityTitle": f"ğŸ“ {url}",
            "activitySubtitle": "**å¤‰æ›´å†…å®¹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå·®åˆ†ï¼‰**",
            "text": f"```\n{text_diff[:1500]}\n```"
        })

    payload = {
        "@type": "MessageCard",
        "@context": "https://schema.org/extensions",
        "summary": f"ç«¶åˆã‚µã‚¤ãƒˆæ›´æ–°æ¤œçŸ¥ ({len(changed_urls)}ä»¶)",
        "themeColor": "0078D4",
        "title": f"ğŸ”” ç«¶åˆã‚µã‚¤ãƒˆæ›´æ–°æ¤œçŸ¥ ({len(changed_urls)}ä»¶)",
        "sections": sections
    }

    try:
        resp = requests.post(TEAMS_WEBHOOK, json=payload, timeout=10)
        resp.raise_for_status()
        print(f"[OK] Teamsé€šçŸ¥é€ä¿¡å®Œäº†")
    except Exception as e:
        print(f"[ERROR] Teamsé€šçŸ¥é€ä¿¡å¤±æ•—: {e}")


def main():
    print(f"[START] {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')} ãƒã‚§ãƒƒã‚¯é–‹å§‹")

    hashes = load_hashes()
    changed = []

    for url in TARGET_URLS:
        current_html = get_page_content(url)
        if current_html is None:
            continue

        # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆHTMLã‚¿ã‚°é™¤å»å¾Œï¼‰
        current_hash = get_text_content_hash(current_html)
        prev_hash = hashes.get(url)

        if prev_hash is None:
            # åˆå›ç™»éŒ²
            print(f"[NEW]     {url}")
            hashes[url] = current_hash
            save_content(url, current_html)
        elif current_hash != prev_hash:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå¤‰æ›´ã•ã‚ŒãŸ
            print(f"[CHANGED] {url}")
            
            old_html = load_content(url)
            text_diff = "å‰å›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            
            if old_html:
                # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºã—ã¦å·®åˆ†ã‚’ä½œæˆ
                old_text = strip_html_tags(old_html)
                new_text = strip_html_tags(current_html)
                text_diff = get_diff_summary(old_text, new_text, max_lines=30)
            
            changed.append({
                "url": url,
                "text_diff": text_diff
            })
            
            hashes[url] = current_hash
            save_content(url, current_html)
        else:
            print(f"[OK]      {url} (ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¤‰æ›´ãªã—)")

    save_hashes(hashes)

    if changed:
        send_teams_alert(changed)
    else:
        print("[INFO] å®Ÿè³ªçš„ãªå¤‰æ›´ãªã—")

    print("[END] ãƒã‚§ãƒƒã‚¯å®Œäº†")


if __name__ == "__main__":
    main()
